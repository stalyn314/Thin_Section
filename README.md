<h1 align="center">Automatic description of rock thin sections: a web application</h1>

<div align="center">
<img src="https://img.shields.io/badge/Transformer-blue?link=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762" alt="Static Badge">
</div>

<p align="center">
  <img width="460" height="300" src="https://github.com/stalyn314/Thin_Section/blob/main/tested_image/T_08495.jpg?raw=true">
</p>


https://shields.io/badges/static-badge


## What are rock thin sections
<p>Image captioning is the process of generating a natural language description of an image. It is a task in the field of computer vision and natural language processing. The goal of image captioning is to generate a coherent and fluent sentence that accurately describes the image content.</p>

An image captioning system typically consists of two main components:

<li>An image feature extractor: This component is responsible for extracting features from the input image, such as object locations, sizes, and colors.

<li>A natural language generator: This component takes the image features as input and generates a natural language description of the image.</li>

<li>The generated captions are typically evaluated using metrics such as BLEU, METEOR, ROUGE, and CIDEr.</li>

## Automatic description (Image Captioning)

<p>Image captioning is the process of generating a natural language description of an image. It is a task in the field of computer vision and natural language processing. The goal of image captioning is to generate a coherent and fluent sentence that accurately describes the image content.</p>

An image captioning system typically consists of two main components:

<li>An image feature extractor: This component is responsible for extracting features from the input image, such as object locations, sizes, and colors.

<li>A natural language generator: This component takes the image features as input and generates a natural language description of the image.</li>

<li>The generated captions are typically evaluated using metrics such as BLEU, METEOR, ROUGE, and CIDEr.</li>

### Exit Anaconda Environment

```
conda deactivate
```
## Note
Training model file is here: https://colab.research.google.com/drive/1K2ZFaAUNIYV0L92XEsV56HSYaXi4DMDh?usp=sharing. I use this file to train model and save its weights to local computer to deploy in Streamlit (You can find it at model/model_IC.h5).

The model tutorial: https://keras.io/examples/vision/image_captioning/

You can read the submitted report to understand the process I do this project.

Feel free to clone my code to use.

